{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504896ef",
   "metadata": {},
   "source": [
    "## Provided code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255d8f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai_clip in /opt/anaconda3/envs/ml/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: ftfy in /opt/anaconda3/envs/ml/lib/python3.12/site-packages (from openai_clip) (6.3.1)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/envs/ml/lib/python3.12/site-packages (from openai_clip) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/ml/lib/python3.12/site-packages (from openai_clip) (4.67.1)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/ml/lib/python3.12/site-packages (from ftfy->openai_clip) (0.2.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e47a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10b662550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "# set random seed for reproducibility\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d427e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be stored.\n",
    "        transform (torch.Compose)\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
    "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
    "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return train, val, test\n",
    "\n",
    "def base_novel_categories(dataset):\n",
    "    # set returns the unique set of all dataset classes\n",
    "    all_classes = set(dataset._labels)\n",
    "    # and let's count them\n",
    "    num_classes = len(all_classes)\n",
    "\n",
    "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
    "    # then we slice the list in half and generate base and novel category lists\n",
    "    base_classes = list(range(num_classes))[:num_classes//2]\n",
    "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
    "    return base_classes, novel_classes\n",
    "\n",
    "def split_data(dataset, base_classes):\n",
    "    \"\"\"Split dataset into base and novel categories based on provided base classes.\n",
    "    Args:\n",
    "        dataset (torch.utils.data.Dataset): The dataset to split.\n",
    "        base_classes (list): List of base class indices.\n",
    "    Returns:\n",
    "        tuple: A tuple containing two subsets of the dataset:\n",
    "            - base_dataset: Subset containing samples from base classes.\n",
    "            - novel_dataset: Subset containing samples from novel classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # these two lists will store the sample indexes\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "\n",
    "    # we create a set of base classes to compute the test below in O(1)\n",
    "    # this is optional and can be removed\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    # here we iterate over sample labels and also get the correspondent sample index\n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    # here we create the dataset subsets\n",
    "    # the torch Subset is just a wrapper around the dataset\n",
    "    # it simply stores the subset indexes and the original dataset (your_subset.dataset)\n",
    "    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it\n",
    "    # https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "    return base_dataset, novel_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "278c5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our flower names (manually defined)\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load the dataset and apply the CLIP transform\n",
    "train_set, val_set, test_set = get_data(transform=clip.load(\"ViT-B/16\")[1])\n",
    "\n",
    "# Split the dataset into base and novel categories\n",
    "base_classes, novel_classes = base_novel_categories(train_set)\n",
    "\n",
    "#split the three datasets into base and novel categories\n",
    "train_base, _= split_data(train_set, base_classes)\n",
    "val_base, _ = split_data(val_set, base_classes)\n",
    "test_base, test_novel = split_data(test_set, base_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10159a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding\n",
    "        x = x.permute(1, 0, 2)  # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "    \n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "\n",
    "        # Use given words to initialize context vectors\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init).to(clip_model.token_embedding.weight.device)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt)\n",
    "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            if csc:\n",
    "                print(\"Initializing class-specific contexts\")\n",
    "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim)\n",
    "            else:\n",
    "                print(\"Initializing a generic context\")\n",
    "                ctx_vectors = torch.empty(n_ctx, ctx_dim)\n",
    "\n",
    "            torch.nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        print(f\"Initial context: '{prompt_prefix}'\")\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        # These are the `prompts` we want to optimize\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(clip_model.token_embedding.weight.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
    "\n",
    "        # These token vectors will be saved when in save_model(),\n",
    "        # but they should be ignored in load_model() as we want to use\n",
    "        # those computed using the current class names\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
    "\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "        self.name_lens = name_lens\n",
    "        self.class_token_position = class_token_position\n",
    "\n",
    "    def forward(self):\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx\n",
    "\n",
    "        # If CoOp, expand the ctx for all classes\n",
    "        if ctx.dim() == 2:\n",
    "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "\n",
    "        if self.class_token_position == \"end\":\n",
    "            prompts = torch.cat(\n",
    "                [\n",
    "                    prefix,  # (n_cls, 1, dim)\n",
    "                    ctx,     # (n_cls, n_ctx, dim)\n",
    "                    suffix,  # (n_cls, *, dim)\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        elif self.class_token_position == \"middle\":\n",
    "            half_n_ctx = self.n_ctx // 2\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i + 1, :, :]\n",
    "                class_i = suffix[i : i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
    "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
    "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,     # (1, 1, dim)\n",
    "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
    "                        class_i,      # (1, name_len, dim)\n",
    "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
    "                        suffix_i,     # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        elif self.class_token_position == \"front\":\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i + 1, :, :]\n",
    "                class_i = suffix[i : i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
    "                ctx_i = ctx[i : i + 1, :, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,  # (1, 1, dim)\n",
    "                        class_i,   # (1, name_len, dim)\n",
    "                        ctx_i,     # (1, n_ctx, dim)\n",
    "                        suffix_i,  # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        return prompts\n",
    "    \n",
    "class OurCLIP(nn.Module):\n",
    "    def __init__(self, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
    "        super().__init__()\n",
    "        clip_model, _ = clip.load(\"ViT-B/16\")\n",
    "        # clip_model = clip_model.cpu()\n",
    "        clip_model = clip_model.float()\n",
    "\n",
    "        self.clip = clip_model\n",
    "        self.clip.eval()\n",
    "        self.prompt_learner = PromptLearner(clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=csc)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        \n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.image_encoder(image)\n",
    "\n",
    "        prompts = self.prompt_learner()\n",
    "        tokenized_prompts = self.tokenized_prompts\n",
    "        text_features = self.text_encoder(prompts, tokenized_prompts)\n",
    "\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits, text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f96354",
   "metadata": {},
   "source": [
    "## My code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b78cf7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptMLP(nn.Module):\n",
    "    def __init__(self, n_ctx=4, hidden=1024):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(512, hidden, bias=False)\n",
    "        self.ln1 = nn.LayerNorm(hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden, bias=False)\n",
    "        self.ln2 = nn.LayerNorm(hidden)\n",
    "        self.out = nn.Linear(hidden, n_ctx * 512, bias=False)\n",
    "        self.n_ctx = n_ctx\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.float()\n",
    "        h = self.ln1(F.gelu(self.fc1(x)))\n",
    "        h = self.ln2(F.gelu(self.fc2(h)))\n",
    "        p = self.out(h).view(-1, self.n_ctx, 512)\n",
    "        return F.normalize(p, dim=-1)   # keep same scale as Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d87ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All our hyperparameters and configurations\n",
    "batch_size = 16\n",
    "num_classes=102\n",
    "device=torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "learning_rate=0.002\n",
    "weight_decay=0.0005\n",
    "momentum=0.9\n",
    "epochs=3\n",
    "epochs_MetaMLP=75\n",
    "run_name=\"experiment\"\n",
    "n_ctx=4\n",
    "ctx_init=\"\"\n",
    "class_token_position=\"end\"\n",
    "csc=True\n",
    "Î» = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62a4635a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing class-specific contexts\n",
      "Initial context: 'X X X X'\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both image and text encoders\n",
      "Number of trainable parameters for prompts: 208896\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:24<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3253, CE Loss: 1.3253, DIST Loss: 0.7398\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:21<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2399, CE Loss: 0.2399, DIST Loss: 0.7706\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:22<00:00,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0991, CE Loss: 0.0991, DIST Loss: 0.7841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_base, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "val_loader = torch.utils.data.DataLoader(val_base, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "test_base_loader = torch.utils.data.DataLoader(test_base, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "test_novel_loader = torch.utils.data.DataLoader(test_novel, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "net = OurCLIP(\n",
    "        classnames=CLASS_NAMES[:num_classes],\n",
    "        n_ctx=n_ctx,\n",
    "        ctx_init=ctx_init,\n",
    "        class_token_position=class_token_position,\n",
    "        csc=csc,\n",
    "    ).to(device)\n",
    "\n",
    "print(\"Turning off gradients in both image and text encoders\")\n",
    "for name, param in net.named_parameters():\n",
    "    if \"prompt_learner\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "print(f\"Number of trainable parameters for prompts: {sum(p.numel() for p in net.parameters() if p.requires_grad)}\")\n",
    "optimizer = torch.optim.SGD([{\"params\": net.parameters()}], lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "handcrafted_all_tokenized = clip.tokenize([f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in base_classes + novel_classes]).to(device)\n",
    "with torch.no_grad():\n",
    "    ref_text_feats = net.clip.encode_text(handcrafted_all_tokenized).float()   # (C, D)\n",
    "    ref_text_feats = ref_text_feats / ref_text_feats.norm(dim=-1, keepdim=True)\n",
    "\n",
    "ref_text_feats = ref_text_feats.detach()\n",
    "\n",
    "criterion_mse = nn.MSELoss()  \n",
    "criterion_ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    running_dist = 0.0\n",
    "    running_ce = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, text_features = net(images)\n",
    "        ce_loss = criterion_ce(logits, labels)\n",
    "        dist_loss = torch.norm(text_features - ref_text_feats, dim=-1).mean()\n",
    "        loss = ce_loss + Î» * dist_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_dist += dist_loss.item()\n",
    "        running_ce += ce_loss.item()\n",
    "    print(f\"Loss: {running_loss / len(train_loader):.4f}, CE Loss: {running_ce / len(train_loader):.4f}, DIST Loss: {running_dist / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63040e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test-accuracy after training from the base training set, but BEFORE applying the PromptMLP:\")\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(test_base_loader, desc=\"Testing on base classes\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits, _ = net(images)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f\"Base classes accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(test_novel_loader, desc=\"Testing on novel classes\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits, _ = net(images)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f\"Novel classes accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65720e29",
   "metadata": {},
   "source": [
    "NO CONTEXT SPECIFIC PROMPTS\n",
    "batch_size = 16\n",
    "num_classes=102\n",
    "device=torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "learning_rate=0.002\n",
    "weight_decay=0.0005\n",
    "momentum=0.9\n",
    "epochs=4\n",
    "epochs_MetaMLP=50\n",
    "run_name=\"experiment\"\n",
    "n_ctx=4\n",
    "ctx_init=\"a photo of\"\n",
    "class_token_position=\"end\"\n",
    "csc=False\n",
    "Î» = 0.0\n",
    "acc: 66.32,90.38\n",
    "\n",
    "CONTEXT SPECIFIC PROMPTS\n",
    "batch_size = 16\n",
    "num_classes=102\n",
    "device=torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "learning_rate=0.002\n",
    "weight_decay=0.0005\n",
    "momentum=0.9\n",
    "epochs=4\n",
    "epochs_MetaMLP=50\n",
    "run_name=\"experiment\"\n",
    "n_ctx=4\n",
    "ctx_init=\"\"\n",
    "class_token_position=\"end\"\n",
    "csc=True\n",
    "Î» = 0.0\n",
    "acc: 18.34, 95.43\n",
    "\n",
    "CONTEXT SPECIFIC PROMPTS + MetaLearn\n",
    "(arguments like before)\n",
    "acc: low ,86.9\n",
    "\n",
    "CONTEXT SPECIFIC PROMPTS + MetaLearn + KgCoOp loss\n",
    "\n",
    "CONTEXT SPECIFIC PROMPTS + KgCoOp loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feedc93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Y_orig = net.prompt_learner.ctx[base_classes].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6115379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    5  Loss: 0.747438\n",
      "Epoch   10  Loss: 0.522728\n",
      "Epoch   15  Loss: 0.404996\n",
      "Epoch   20  Loss: 0.297059\n",
      "Epoch   25  Loss: 0.236042\n",
      "Epoch   30  Loss: 0.174119\n",
      "Epoch   35  Loss: 0.137404\n",
      "Epoch   40  Loss: 0.150249\n",
      "Epoch   45  Loss: 0.119394\n",
      "Epoch   50  Loss: 0.103884\n",
      "Epoch   55  Loss: 0.076659\n",
      "Epoch   60  Loss: 0.070431\n",
      "Epoch   65  Loss: 0.099107\n",
      "Epoch   70  Loss: 0.088941\n",
      "Epoch   75  Loss: 0.103399\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP model and move to device\n",
    "clip_model, _ = clip.load(\"ViT-B/16\")\n",
    "clip_model = clip_model.to(device)\n",
    "\n",
    "# Tokenize prompts for base and all classes\n",
    "handcrafted_base_tokenized = clip.tokenize([f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in base_classes]).to(device)\n",
    "handcrafted_all_tokenized = clip.tokenize([f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in base_classes + novel_classes]).to(device)\n",
    "\n",
    "# ----- Embedding extraction and normalization -----\n",
    "with torch.no_grad():\n",
    "    # X: (base_size, 512) â€“ text embeddings for handcrafted prompts\n",
    "    X = clip_model.encode_text(handcrafted_base_tokenized)\n",
    "    X = F.normalize(X, dim=-1).float()  # unit-norm embeddings\n",
    "\n",
    "    # Y: (base_size, n_ctx, 512) â€“ normalized learned context vectors\n",
    "    Y = F.normalize(Y_orig, dim=-1).float()\n",
    "\n",
    "X, Y = X.detach(), Y.detach()  # Detach from computation graph\n",
    "\n",
    "# ----- Train PromptMLP -----\n",
    "promptMLP = PromptMLP(n_ctx=n_ctx).to(device)\n",
    "optimizer = torch.optim.AdamW(promptMLP.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=400)\n",
    "\n",
    "# Cosine-style loss (scale-invariant)\n",
    "drop_p = 0.1                 # probability of masking EACH token\n",
    "eps    = 1e-6                   # for safe normalisation\n",
    "\n",
    "for epoch in range(75):\n",
    "    optimizer.zero_grad()\n",
    "    pred = promptMLP(X)           # (B, n_ctx, 512)  **already unit-norm**\n",
    "\n",
    "    # â”€â”€ ðŸ”„ TOKEN-DROPOUT  START â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    mask = (torch.rand(pred.shape[:2], device=pred.device) > drop_p)  # (B, n_ctx)\n",
    "    single_token_fix = mask.sum(dim=1, keepdim=True) == 0\n",
    "    mask = mask | single_token_fix\n",
    "    mask = mask.unsqueeze(-1)                                         # (B, n_ctx, 1)\n",
    "\n",
    "    pred_m = pred * mask\n",
    "    Y_m    = Y    * mask\n",
    "\n",
    "    pred_m = F.normalize(pred_m, dim=-1, eps=eps)\n",
    "    Y_m    = F.normalize(Y_m,    dim=-1, eps=eps)\n",
    "\n",
    "    loss = 1 - (pred_m * Y_m).sum(-1).mean()\n",
    "    # â”€â”€ ðŸ”„ TOKEN-DROPOUT  END â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch + 1:4d}  Loss: {loss.item():.6f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedded_all = clip_model.encode_text(handcrafted_all_tokenized)\n",
    "    embedded_all = F.normalize(embedded_all, dim=-1).float()\n",
    "    # Use mean norm of original Y to scale the predicted context vectors\n",
    "    net.prompt_learner.ctx = nn.Parameter(promptMLP(embedded_all) * Y_orig.norm(dim=-1, keepdim=True).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8813c5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdc5adbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-accuracy after training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on base classes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [02:39<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base classes accuracy: 84.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on novel classes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [03:37<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novel classes accuracy: 70.48%\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test-accuracy after training:\")\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(test_base_loader, desc=\"Testing on base classes\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits, _ = net(images)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f\"Base classes accuracy: {100 * correct / total:.2f}%\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(test_novel_loader, desc=\"Testing on novel classes\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits, _ = net(images)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f\"Novel classes accuracy: {100 * correct / total:.2f}%\")\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4cbaf",
   "metadata": {},
   "source": [
    "My results (seed=0):\n",
    "epochs=75, drop_p=0.1, lr=3e-4, weight_decay=1e-2: 84.59/68.25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c4ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model and move to device\n",
    "clip_model, _ = clip.load(\"ViT-B/16\")\n",
    "clip_model = clip_model.to(device)\n",
    "\n",
    "# ----- Tokenization -----\n",
    "# TEMPLATES = [\n",
    "#     \"a photo of a {}, a type of flower\",\n",
    "#     \"a macro shot of a {} flower\",\n",
    "#     \"a high-resolution image of a blooming {}\",\n",
    "#     \"a natural photograph of a {} in focus\",\n",
    "#     \"a vibrant picture of a {} with petals visible\",\n",
    "#     \"a botanical image showing a {} up close\",\n",
    "#     \"a centered composition of a {} in daylight\",\n",
    "#     \"a single {} flower captured in detail\"\n",
    "# ]\n",
    "\n",
    "# def generate_prompts(class_ids):\n",
    "#     prompts = []\n",
    "#     for c in class_ids:\n",
    "#         for template in TEMPLATES:\n",
    "#             prompts.append(template.format(CLASS_NAMES[c]))\n",
    "#     return prompts\n",
    "\n",
    "def generate_prompts(class_ids):\n",
    "    return [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in class_ids]\n",
    "\n",
    "# Tokenize prompts for base and all classes\n",
    "handcrafted_base_tokenized = clip.tokenize(generate_prompts(base_classes)).to(device)\n",
    "handcrafted_all_tokenized = clip.tokenize(generate_prompts(base_classes + novel_classes)).to(device)\n",
    "\n",
    "# ----- Embedding extraction and normalization -----\n",
    "with torch.no_grad():\n",
    "    # X: (base_size, 512) â€“ text embeddings for handcrafted prompts\n",
    "    X = clip_model.encode_text(handcrafted_base_tokenized)\n",
    "    X = F.normalize(X, dim=-1).float()  # unit-norm embeddings\n",
    "\n",
    "    # Y: (base_size, n_ctx, 512) â€“ normalized learned context vectors\n",
    "    Y_orig = net.prompt_learner.ctx[base_classes].clone()\n",
    "    Y = F.normalize(Y_orig, dim=-1).float()\n",
    "\n",
    "X, Y = X.detach(), Y.detach()  # Detach from computation graph\n",
    "\n",
    "# ----- Train PromptMLP -----\n",
    "promptMLP = PromptMLP(n_ctx=n_ctx).to(device)\n",
    "optimizer = torch.optim.AdamW(promptMLP.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=400)\n",
    "\n",
    "# Cosine-style loss (scale-invariant)\n",
    "for epoch in range(epochs_MetaMLP):\n",
    "    optimizer.zero_grad()\n",
    "    pred = promptMLP(X)  # shape: (base_size, n_ctx, 512), already unit-norm\n",
    "    loss = 1 - (pred * Y).sum(-1).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"epoch {epoch + 1:4d}  loss {loss.item():.6f}\")\n",
    "\n",
    "# ----- Inference: Use trained PromptMLP to update context vectors -----\n",
    "with torch.no_grad():\n",
    "    embedded_all = clip_model.encode_text(handcrafted_all_tokenized)\n",
    "    embedded_all = F.normalize(embedded_all, dim=-1).float()\n",
    "    # Use mean norm of original Y to scale the predicted context vectors\n",
    "    net.prompt_learner.ctx = nn.Parameter(promptMLP(embedded_all) * Y_orig.norm(dim=-1, keepdim=True).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
