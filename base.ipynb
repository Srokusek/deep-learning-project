{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f92426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.dataset import get_data, base_novel_categories, split_dataset, CLASS_NAMES\n",
    "from src.models.clip_wrapper import load_clip_model\n",
    "from src.training.evaluation import eval, linear_probe_evaluation\n",
    "from src.utils.metrics import harmonic_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63917d2b",
   "metadata": {},
   "source": [
    "# CLIP base performance on flowers102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28021c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess, device = load_clip_model(model_name=\"ViT-B/16\")\n",
    "\n",
    "train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "\n",
    "base_classes, novel_classes = base_novel_categories(train_set)\n",
    "\n",
    "train_base, train_novel = split_dataset(train_set, base_classes)\n",
    "val_base, _ = split_dataset(val_set, base_classes)\n",
    "test_base, test_novel = split_dataset(test_set, base_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33acb089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Zero-shot evaluation on Base Classes: 100%|██████████| 20/20 [00:08<00:00,  2.49it/s]\n",
      "🧠 Zero-shot evaluation on Novel Classes: 100%|██████████| 29/29 [00:11<00:00,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Base classes accuracy: 71.25%\n",
      "🔍 Novel classes accuracy: 78.26%\n",
      "🔍 Harmonic Mean: 74.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_accuracy = eval(model=model, dataset=test_base, categories=base_classes, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Base Classes\")\n",
    "novel_accuracy = eval(model=model, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Novel Classes\")\n",
    "\n",
    "print()\n",
    "print(f\"🔍 Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
    "print(f\"🔍 Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n",
    "\n",
    "print(f\"🔍 Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54b6fb4",
   "metadata": {},
   "source": [
    "# Linear Probe Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f9bdfb",
   "metadata": {},
   "source": [
    "The purpose of linear probe evaluation is to retrieve the features (embeddings) the model makes for the dataset of interest. We collect the labels and embeddings and then try to fit a simple classifier (usually logistic regression or a single linear layer + softmax). Evaluating the performance of this simple model gives an idea of how well the network is able to seperate the categories in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "263d035f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear_probe_evaluation() missing 1 required positional argument: 'test_set'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlinear_probe_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: linear_probe_evaluation() missing 1 required positional argument: 'test_set'"
     ]
    }
   ],
   "source": [
    "linear_probe_evaluation(train_base, test_base, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa8ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_probe_evaluation(train_novel, test_novel, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
