{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e38dd758",
   "metadata": {},
   "source": [
    "Provided code from the lab session and the assignment notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae249520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai_clip in /opt/anaconda3/envs/ml/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: ftfy in /opt/anaconda3/envs/ml/lib/python3.12/site-packages (from openai_clip) (6.3.1)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/envs/ml/lib/python3.12/site-packages (from openai_clip) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/ml/lib/python3.12/site-packages (from openai_clip) (4.67.1)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/ml/lib/python3.12/site-packages (from ftfy->openai_clip) (0.2.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a5278e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10febe550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "# set random seed for reproducibility\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4223b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be stored.\n",
    "        transform (torch.Compose)\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
    "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
    "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return train, val, test\n",
    "\n",
    "def base_novel_categories(dataset):\n",
    "    # set returns the unique set of all dataset classes\n",
    "    all_classes = set(dataset._labels)\n",
    "    # and let's count them\n",
    "    num_classes = len(all_classes)\n",
    "\n",
    "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
    "    # then we slice the list in half and generate base and novel category lists\n",
    "    base_classes = list(range(num_classes))[:num_classes//2]\n",
    "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
    "    return base_classes, novel_classes\n",
    "\n",
    "def split_data(dataset, base_classes):\n",
    "    \"\"\"Split dataset into base and novel categories based on provided base classes.\n",
    "    Args:\n",
    "        dataset (torch.utils.data.Dataset): The dataset to split.\n",
    "        base_classes (list): List of base class indices.\n",
    "    Returns:\n",
    "        tuple: A tuple containing two subsets of the dataset:\n",
    "            - base_dataset: Subset containing samples from base classes.\n",
    "            - novel_dataset: Subset containing samples from novel classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # these two lists will store the sample indexes\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "\n",
    "    # we create a set of base classes to compute the test below in O(1)\n",
    "    # this is optional and can be removed\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    # here we iterate over sample labels and also get the correspondent sample index\n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    # here we create the dataset subsets\n",
    "    # the torch Subset is just a wrapper around the dataset\n",
    "    # it simply stores the subset indexes and the original dataset (your_subset.dataset)\n",
    "    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it\n",
    "    # https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "    return base_dataset, novel_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0186a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our flower names (manually defined)\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load the dataset and apply the CLIP transform\n",
    "train_set, val_set, test_set = get_data(transform=clip.load(\"ViT-B/16\")[1])\n",
    "\n",
    "# Split the dataset into base and novel categories\n",
    "base_classes, novel_classes = base_novel_categories(train_set)\n",
    "\n",
    "#split the three datasets into base and novel categories\n",
    "train_base, _= split_data(train_set, base_classes)\n",
    "val_base, _ = split_data(val_set, base_classes)\n",
    "test_base, test_novel = split_data(test_set, base_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e8b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding\n",
    "        x = x.permute(1, 0, 2)  # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "    \n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "\n",
    "        # Use given words to initialize context vectors\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init).to(clip_model.token_embedding.weight.device)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt)\n",
    "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            if csc:\n",
    "                # print(\"Initializing class-specific contexts\")\n",
    "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim)\n",
    "            else:\n",
    "                # print(\"Initializing a generic context\")\n",
    "                ctx_vectors = torch.empty(n_ctx, ctx_dim)\n",
    "\n",
    "            torch.nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        # print(f\"Initial context: '{prompt_prefix}'\")\n",
    "        # print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        # These are the `prompts` we want to optimize\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(clip_model.token_embedding.weight.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
    "\n",
    "        # These token vectors will be saved when in save_model(),\n",
    "        # but they should be ignored in load_model() as we want to use\n",
    "        # those computed using the current class names\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
    "\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "        self.name_lens = name_lens\n",
    "        self.class_token_position = class_token_position\n",
    "\n",
    "    def forward(self):\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx\n",
    "\n",
    "        # If CoOp, expand the ctx for all classes\n",
    "        if ctx.dim() == 2:\n",
    "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "\n",
    "        if self.class_token_position == \"end\":\n",
    "            prompts = torch.cat(\n",
    "                [\n",
    "                    prefix,  # (n_cls, 1, dim)\n",
    "                    ctx,     # (n_cls, n_ctx, dim)\n",
    "                    suffix,  # (n_cls, *, dim)\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        elif self.class_token_position == \"middle\":\n",
    "            half_n_ctx = self.n_ctx // 2\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i + 1, :, :]\n",
    "                class_i = suffix[i : i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
    "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
    "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,     # (1, 1, dim)\n",
    "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
    "                        class_i,      # (1, name_len, dim)\n",
    "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
    "                        suffix_i,     # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        elif self.class_token_position == \"front\":\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i + 1, :, :]\n",
    "                class_i = suffix[i : i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
    "                ctx_i = ctx[i : i + 1, :, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,  # (1, 1, dim)\n",
    "                        class_i,   # (1, name_len, dim)\n",
    "                        ctx_i,     # (1, n_ctx, dim)\n",
    "                        suffix_i,  # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        return prompts\n",
    "    \n",
    "class OurCLIP(nn.Module):\n",
    "    def __init__(self, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
    "        super().__init__()\n",
    "        clip_model, _ = clip.load(\"ViT-B/16\")\n",
    "        \n",
    "        clip_model = clip_model.float()\n",
    "\n",
    "        self.clip = clip_model\n",
    "        self.clip.eval()\n",
    "        self.prompt_learner = PromptLearner(clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=csc)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        \n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.image_encoder(image)\n",
    "\n",
    "        prompts = self.prompt_learner()\n",
    "        tokenized_prompts = self.tokenized_prompts\n",
    "        text_features = self.text_encoder(prompts, tokenized_prompts)\n",
    "\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits, text_features\n",
    "    \n",
    "\n",
    "def harmonic_mean(base_accuracy, novel_accuracy):\n",
    "    numerator = 2\n",
    "    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
    "    hm = numerator / denominator\n",
    "    return hm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377fb939",
   "metadata": {},
   "source": [
    "Our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20300aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptMLP(nn.Module):\n",
    "    def __init__(self, n_ctx=4, hidden=1024):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(512, hidden, bias=False)\n",
    "        self.ln1 = nn.LayerNorm(hidden)\n",
    "        self.out = nn.Linear(hidden, n_ctx * 512, bias=False)\n",
    "        self.n_ctx = n_ctx\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.float()\n",
    "        h = self.ln1(F.gelu(self.fc1(x)))\n",
    "        p = self.out(h).view(-1, self.n_ctx, 512)\n",
    "        return F.normalize(p, dim=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c82f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, test_classes, learned_prompts, evaluation_type=\"\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    contig_cat2idx = {cat: idx for idx, cat in enumerate(test_classes)}\n",
    "    \n",
    "    if not learned_prompts:\n",
    "        handcrafted_tokenized = clip.tokenize([f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in test_classes]).to(device)\n",
    "        text_features = model.encode_text(handcrafted_tokenized)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            labels = torch.Tensor([contig_cat2idx[t.item()] for t in labels]).long().to(device)\n",
    "            if learned_prompts == False:\n",
    "                image_features = model.encode_image(images)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                predicted = (image_features @ text_features.T).argmax(dim=-1)\n",
    "            else:\n",
    "                logits, _ = model(images)\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(f\"Accuracy for {evaluation_type}: {100 * correct / total:.2f}%\")\n",
    "        \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # Here we set general hyperparameters.\n",
    "    batch_size = 16\n",
    "    training_classes = base_classes\n",
    "    device=torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    # Here we set hyperparameters for the training of the CoOp PromptLearner.\n",
    "    learning_rate=0.002\n",
    "    weight_decay=0.0005\n",
    "    momentum=0.9\n",
    "    epochs=3\n",
    "    n_ctx=4\n",
    "    ctx_init=\"\"\n",
    "    class_token_position=\"end\"\n",
    "    csc=True # Needs to be true for our method to work\n",
    "    λ = 8.0 # This hyperparameter is for the use of KgCoOp. As described in their paper, a value of 8 worked best for their implementation, which is why we also use it here.\n",
    "\n",
    "    # Here we hyperparameters for PromptMLP training.\n",
    "    lr=3e-4\n",
    "    weight_decay=1e-2\n",
    "    epochs_MetaMLP=75\n",
    "    T_max = 400\n",
    "    csc=True\n",
    "    drop_p = 0.05                   \n",
    "    eps    = 1e-6  \n",
    "\n",
    "    # Set the data loader for train and test time.\n",
    "    train_loader = torch.utils.data.DataLoader(train_base, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    # val_loader = torch.utils.data.DataLoader(val_base, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    test_base_loader = torch.utils.data.DataLoader(test_base, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    test_novel_loader = torch.utils.data.DataLoader(test_novel, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    # net_base will be the model performing on the base dataset. It learns the class-specific prompts for our base classes.\n",
    "    net_base = OurCLIP(\n",
    "        classnames=[CLASS_NAMES[i] for i in training_classes],\n",
    "        n_ctx=n_ctx,\n",
    "        ctx_init=ctx_init,\n",
    "        class_token_position=class_token_position,\n",
    "        csc=csc,\n",
    "    ).to(device)\n",
    "\n",
    "    # Here we remap our labels into a contiguous set staring from zero.\n",
    "    contig_cat2idx = {cat: idx for idx, cat in enumerate(base_classes)}\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Training the class specific prompts with CoOp for base classes:\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # We freeze all the other parameters that are not the context prompts.\n",
    "    print(\"Turning off gradients in both image and text encoders\")\n",
    "    for name, param in net_base.named_parameters():\n",
    "        if \"prompt_learner\" not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # We print the number of parameters.\n",
    "    print(f\"Number of trainable parameters for prompts: {sum(p.numel() for p in net_base.parameters() if p.requires_grad)}\")\n",
    "    print()\n",
    "    optimizer = torch.optim.SGD([{\"params\": net_base.parameters()}], lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    # This section is for applying KgCoOp like loss.\n",
    "    # KgCoOp take the output of the text encoder of handcrafted class prompts. In KgCoOp the euclidean distance between these embeddings and the embeddings with the learned context vectors is minimized.\n",
    "    handcrafted_all_tokenized = clip.tokenize([f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in base_classes]).to(device)\n",
    "    with torch.no_grad():\n",
    "        ref_text_feats = net_base.clip.encode_text(handcrafted_all_tokenized).float() \n",
    "        ref_text_feats = ref_text_feats / ref_text_feats.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    ref_text_feats = ref_text_feats.detach()\n",
    "\n",
    "    # Our regular Cross-Entropy loss function for classification.\n",
    "    criterion_ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # In this loop we train the CoOp model. Very base class receives a corresponding learned prompt individually.\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        net_base.train()\n",
    "        running_loss = 0.0\n",
    "        running_dist = 0.0\n",
    "        running_ce = 0.0\n",
    "        for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            images = images.to(device)\n",
    "            labels = torch.Tensor([contig_cat2idx[t.item()] for t in labels]).long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits, text_features = net_base(images)\n",
    "            ce_loss = criterion_ce(logits, labels)\n",
    "            dist_loss = torch.norm(text_features - ref_text_feats, dim=-1).mean()\n",
    "            loss = ce_loss + λ * dist_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_dist += dist_loss.item()\n",
    "            running_ce += ce_loss.item()\n",
    "        print(f\"Loss: {running_loss / len(train_loader):.4f}, CE Loss: {running_ce / len(train_loader):.4f}, DIST Loss: {running_dist / len(train_loader):.4f}\")\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Training the PromptMLP to map handcrafted text embeddings of novel classes to novel class prompts:\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # We get the clip_model for base line and also to encode our input values (The embeddings of our handcrafted prompts) for our PromptMLP\n",
    "    clip_model, _ = clip.load(\"ViT-B/16\")\n",
    "    clip_model = clip_model.to(device)\n",
    "\n",
    "    # We define the tokenized versions of both handcrafted base and novel class prompts.\n",
    "    handcrafted_base_tokenized = clip.tokenize([f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in base_classes]).to(device)\n",
    "    handcrafted_novel_tokenized = clip.tokenize([f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in novel_classes]).to(device)\n",
    "\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Our input to the MLP during training are going to be the text features of the handcrafted prompts for base classes.\n",
    "        X = clip_model.encode_text(handcrafted_base_tokenized)\n",
    "        X = F.normalize(X, dim=-1).float()  \n",
    "\n",
    "        # Our desired output/target of the Prompt MLP ae the context tokens of the base classes, whcih were learned by the CoOp Prompt Learner.\n",
    "        Y_orig = net_base.prompt_learner.ctx[base_classes].clone()\n",
    "        Y = F.normalize(Y_orig, dim=-1).float()\n",
    "\n",
    "    X, Y = X.detach(), Y.detach()  \n",
    "\n",
    "    # We define our PromptMLP and an optimizer and a scheduler for training the PromptMLP.\n",
    "    promptMLP = PromptMLP(n_ctx=n_ctx).to(device)\n",
    "    optimizer = torch.optim.AdamW(promptMLP.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)                \n",
    "\n",
    "    for epoch in range(epochs_MetaMLP):\n",
    "        optimizer.zero_grad()\n",
    "        pred = promptMLP(X)         \n",
    "\n",
    "        # Here we apply drop-out of tokens during traing (the target tokens) to prevent overfitting to the base classes\n",
    "        mask = (torch.rand(pred.shape[:2], device=pred.device) > drop_p)  \n",
    "        single_token_fix = mask.sum(dim=1, keepdim=True) == 0\n",
    "        mask = mask | single_token_fix\n",
    "        mask = mask.unsqueeze(-1)                                        \n",
    "\n",
    "        pred_m = pred * mask\n",
    "        Y_m    = Y    * mask\n",
    "\n",
    "        pred_m = F.normalize(pred_m, dim=-1, eps=eps)\n",
    "        Y_m    = F.normalize(Y_m,    dim=-1, eps=eps)\n",
    "\n",
    "        loss = 1 - (pred_m * Y_m).sum(-1).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch + 1:4d}  Loss: {loss.item():.6f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Now we gather our predictions of the prompts for the novel (untrained) classes to get also reasonable context tokens for them\n",
    "        embedded_novel = clip_model.encode_text(handcrafted_novel_tokenized)\n",
    "        embedded_novel = F.normalize(embedded_novel, dim=-1).float()\n",
    "        \n",
    "        # For inference we need a model where we can store the learned context tokens.\n",
    "        # Note that we reuse the structure of the CoOp model with the Prompt Learner to get the correct prefix and suffix for our learned prompts also for the novel classes.\n",
    "        # But we do not train this model further in any way.\n",
    "        net_novel = OurCLIP(\n",
    "            classnames=[CLASS_NAMES[i] for i in novel_classes],\n",
    "            n_ctx=n_ctx,\n",
    "            ctx_init=ctx_init,\n",
    "            class_token_position=class_token_position,\n",
    "            csc=csc,\n",
    "        ).to(device)\n",
    "        net_novel.prompt_learner.ctx = nn.Parameter(promptMLP(embedded_novel) * Y_orig.norm(dim=-1, keepdim=True).mean())\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        # Here we evaluate the base and novel class performance of both the baseline CLIP model, and our adaption.\n",
    "        # Note that the accuracy for the base classes is based on the directly learned context tokens for the base classes through CoOp, while the novel class accuracy is based on the predicted context embeddings adapted by the PromptMLP.\n",
    "        acc_base_baseline = evaluate_model(clip_model, test_base_loader, base_classes, False, \"Base classes - Baseline CLIP\")\n",
    "        acc_novel_baseline = evaluate_model(clip_model, test_novel_loader, novel_classes, False, \"Novel classes - Baseline CLIP\")\n",
    "        acc_base_adapted = evaluate_model(net_base , test_base_loader, base_classes, True, \"Base classes - Our CLIP adapter (here regular CoOp)\")\n",
    "        acc_novel_adapted = evaluate_model(net_novel, test_novel_loader, novel_classes, True, \"Novel classes - Our CLIP adapter with prompts from our PromptMLP\")\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        # And we calculate the Harmonic mean.\n",
    "        print(f\"🔍 Harmonic Mean for the baseline: {harmonic_mean(acc_base_baseline, acc_novel_baseline)*100:.2f}%\")\n",
    "        print()\n",
    "        print(f\"🔍 Harmonic Mean for our adaption: {harmonic_mean(acc_base_adapted, acc_novel_adapted)*100:.2f}%\")\n",
    "        print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b617d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training the class specific prompts with CoOp for base classes:\n",
      "------------------------------------------------------------\n",
      "Turning off gradients in both image and text encoders\n",
      "Number of trainable parameters for prompts: 104448\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 32/32 [01:07<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.6516, CE Loss: 1.1721, DIST Loss: 0.5599\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 32/32 [01:07<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.6520, CE Loss: 0.3201, DIST Loss: 0.4165\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 32/32 [01:08<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.0569, CE Loss: 0.2655, DIST Loss: 0.3489\n",
      "============================================================\n",
      "Training the PromptMLP to map handcrafted text embeddings of novel classes to novel class prompts:\n",
      "------------------------------------------------------------\n",
      "Epoch    5  Loss: 0.775196\n",
      "Epoch   10  Loss: 0.612891\n",
      "Epoch   15  Loss: 0.468715\n",
      "Epoch   20  Loss: 0.367890\n",
      "Epoch   25  Loss: 0.272096\n",
      "Epoch   30  Loss: 0.202493\n",
      "Epoch   35  Loss: 0.160985\n",
      "Epoch   40  Loss: 0.131367\n",
      "Epoch   45  Loss: 0.097936\n",
      "Epoch   50  Loss: 0.077596\n",
      "Epoch   55  Loss: 0.038589\n",
      "Epoch   60  Loss: 0.040989\n",
      "Epoch   65  Loss: 0.046041\n",
      "Epoch   70  Loss: 0.033463\n",
      "Epoch   75  Loss: 0.075837\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 155/155 [01:40<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Base classes - Baseline CLIP: 71.49%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 230/230 [02:07<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Novel classes - Baseline CLIP: 78.40%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 155/155 [02:12<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Base classes - Our CLIP adapter (here regular CoOp): 92.07%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 230/230 [02:52<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Novel classes - Our CLIP adapter with prompts from our PromptMLP: 71.55%\n",
      "\n",
      "============================================================\n",
      "🔍 Harmonic Mean for the baseline: 74.79%\n",
      "\n",
      "🔍 Harmonic Mean for our adaption: 80.52%\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
