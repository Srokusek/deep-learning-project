{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2fdac8d",
   "metadata": {},
   "source": [
    "### CoOp\n",
    "notebook intended to try implementing context optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43cc451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "\n",
    "_tokenizer = _Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f27c8b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(torch.nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer #transformer for text embedding\n",
    "        self.positional_embedding = clip_model.positional_embedding #transformers themselves do not consider order of tokens -> use positional embedding\n",
    "        self.ln_final = clip_model.ln_final #layer normalization\n",
    "        self.text_projection = clip_model.text_projection #projection matrix maps the text embeddings into the shared embedding space\n",
    "    \n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding #add positional embedding (same for each sample in batch)\n",
    "        x = x.permute(1, 0, 2) #(batch_size, seq_len, emd_dim) -> (seq_len , batch_size, emd_dim)\n",
    "        x = self.transformer(x) #compute contextualized embeddings for each toekn\n",
    "        x = x.permute(1, 0, 2) #(seq_len , batch_size, emd_dim) -> (batch_size, seq_len, emd_dim)\n",
    "        x = self.ln_final(x) #normalize embeddings via layerNorm\n",
    "\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] #select most important embedding for each sample in batch\n",
    "        x = x @ self.text_projection #project selected embedding into shared embedding space\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a576b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptLearner(torch.nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init).to(clip_model.token_embedding.weight.device) #create tokens out of initial context\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt) #get token embeddings for each token in context (1 (only one ctx_init), sequence_length, embedding_dim)\n",
    "            ctx_vectors = embedding[0, 1 : 1+n_ctx, :] #initialize context vectors (learnable parameters)\n",
    "            #[0,,] -> only one string as input, [,1 : 1+n_ctx,] -> skip special token at position 0 and get the rest of context tokens, [,,:] -> select all of embedding dim\n",
    "            prompt_prefix = ctx_init\n",
    "\n",
    "        else:\n",
    "            if csc: #CoCoOp\n",
    "                print(\"Initializing class-specific contexts\")\n",
    "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim) #context vectors for context token for each class, size of context dim\n",
    "\n",
    "            else: #CoOp\n",
    "                print(\"Initializing generin context\")\n",
    "                ctx_vectors = torch.empty(n_ctx, ctx_dim) #context vector for each token, size of context dim\n",
    "\n",
    "            torch.nn.init.normal_(ctx_vectors, std=0.02) #initialize context with random values (mean=0 std=0.02)\n",
    "            prompt_prefix = \" \".join(\"X\" * n_ctx) #placeholder for prompt prefix (\"X X X {class_name}\")\n",
    "\n",
    "        print(f\"Initial context: '{prompt_prefix}'\")\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        self.ctx = torch.nn.Parameter(ctx_vectors) #initialize from context vectors (may be random or initiated from init_ctx)\n",
    "\n",
    "        #preprocess the class names in a similar manner\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenizer(p) for p in prompts]).to(clip_model.token_embedding.weight.device) #tokenize the prompts and concat back into a single tensor\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts) #get embedding of the entire prompt\n",
    "\n",
    "        #buffer:= part of the state, but not trainable parameters\n",
    "        #-> used in training but not learnable\n",
    "        #saved in save_model(), but ignored in load_model()\n",
    "        #-> we want to use ones created from current class\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :]) #select the first embedding (special token) for all prompts\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx :, :]) #embeddings for all tokens after the context for all prompts\n",
    "\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "        self.name_lens = name_lens\n",
    "        self.class_token_position = class_token_position\n",
    "\n",
    "    def forward(self):\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx\n",
    "\n",
    "        #if CoOp (csc==False), we expand the context tensor to all classes\n",
    "        if ctx.dim() == 2:\n",
    "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, 1)\n",
    "\n",
    "        if self.class_token_position == \"end\":\n",
    "            prompts = torch.cat(\n",
    "                [\n",
    "                    prefix, #(n_cls, 1, dim)\n",
    "                    ctx, #(n_cls, n_ctx, dim)\n",
    "                    suffix, #(n_cls, *, dim)\n",
    "                ],  \n",
    "                dim=1 #concat along each prompt\n",
    "            )\n",
    "\n",
    "        elif self.class_token_position == \"middle\":\n",
    "            half_n_ctx = self.n_ctx // 2\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls): #for each class prompt\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i+1, :, :] #prefix of class i\n",
    "                class_i = suffix[i : i+1, :name_len, :] #name of class i\n",
    "                suffix_i = suffix[i : i+1, name_len:, :] #suffix of class i\n",
    "                ctx_i_half1 = ctx[i : i+1, :half_n_ctx, :] #first half of context (before class name)\n",
    "                ctx_i_half2 = ctx[i : i+1, half_n_ctx:, :] #secodn half of context (after class name)\n",
    "                #Note: we use [i:i+1,...] because this way the resulting tensor keep the same dimension [1,x,y]\n",
    "                #if we used [i,...] instead the resulting tensor would be one dimension lower [x,y]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i, #(1, 1, dim)\n",
    "                        ctx_i_half1, #(1, n_ctx//2, dim)\n",
    "                        class_i, #(1, name_len, dim)\n",
    "                        ctx_i_half2, #(1, n_ctx//2, dim)\n",
    "                        suffix_i, #(1, *, dim)\n",
    "                    ],\n",
    "                    dim=1 #concat along each prompt\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0) #concat along the classes\n",
    "\n",
    "        #very similar process for position == \"front\"\n",
    "        elif self.class_token_position == \"front\":\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i+1, :, :]\n",
    "                class_i = suffix[i : i+1, :name_len, :]\n",
    "                suffix_i = suffix[i : i+1, name_len:, :]\n",
    "                ctx_i = ctx[i : i+1, :, :]\n",
    "\n",
    "                prompt = torch.cat([\n",
    "                    prefix_i,\n",
    "                    class_i,\n",
    "                    ctx,\n",
    "                    suffix_i,\n",
    "                    ],\n",
    "                    dim=1\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b0187",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "getattr expected at least 2 arguments, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: getattr expected at least 2 arguments, got 1"
     ]
    }
   ],
   "source": [
    "class NewCLIP(torch.nn.Module):\n",
    "    def __init__(self, classnames, n_ctx, ctx_init, class_tken_posiiton, csc=False):\n",
    "        super().__init__()\n",
    "        clip_model, _ = clip.load_model(\"ViT-B/16\")\n",
    "        clip_model = clip_model.float()\n",
    "\n",
    "        self.prompt_learner = PromptLearner(clip_model, classnames, n_ctx, ctx_init, class_tken_posiiton, csc=csc)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.image_encoder(image) #encode the image\n",
    "\n",
    "        prompts = self.prompt_learner() #get the formatted prompts\n",
    "        tokenized_prompts = self.tokenized_prompts \n",
    "        text_features = self.text_encoder(prompts, tokenized_prompts) #encode the text\n",
    "\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f002a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    wd,\n",
    "    momentum, \n",
    "):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
