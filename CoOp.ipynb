{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2fdac8d",
   "metadata": {},
   "source": [
    "### CoOp\n",
    "notebook intended to try implementing context optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43cc451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utils.logging import dump_cuda_cache\n",
    "\n",
    "_tokenizer = _Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f27c8b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(torch.nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer #transformer for text embedding\n",
    "        self.positional_embedding = clip_model.positional_embedding #transformers themselves do not consider order of tokens -> use positional embedding\n",
    "        self.ln_final = clip_model.ln_final #layer normalization\n",
    "        self.text_projection = clip_model.text_projection #projection matrix maps the text embeddings into the shared embedding space\n",
    "    \n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding #add positional embedding (same for each sample in batch)\n",
    "        x = x.permute(1, 0, 2) #(batch_size, seq_len, emd_dim) -> (seq_len , batch_size, emd_dim)\n",
    "        x = self.transformer(x) #compute contextualized embeddings for each toekn\n",
    "        x = x.permute(1, 0, 2) #(seq_len , batch_size, emd_dim) -> (batch_size, seq_len, emd_dim)\n",
    "        x = self.ln_final(x) #normalize embeddings via layerNorm\n",
    "\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] #select most important embedding for each sample in batch\n",
    "        x = x @ self.text_projection #project selected embedding into shared embedding space\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a576b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptLearner(torch.nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init).to(clip_model.token_embedding.weight.device) #create tokens out of initial context\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt) #get token embeddings for each token in context (1 (only one ctx_init), sequence_length, embedding_dim)\n",
    "            ctx_vectors = embedding[0, 1 : 1+n_ctx, :] #initialize context vectors (learnable parameters)\n",
    "            #[0,,] -> only one string as input, [,1 : 1+n_ctx,] -> skip special token at position 0 and get the rest of context tokens, [,,:] -> select all of embedding dim\n",
    "            prompt_prefix = ctx_init\n",
    "\n",
    "        else:\n",
    "            if csc: #CoCoOp\n",
    "                print(\"Initializing class-specific contexts\")\n",
    "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim) #context vectors for context token for each class, size of context dim\n",
    "\n",
    "            else: #CoOp\n",
    "                print(\"Initializing generin context\")\n",
    "                ctx_vectors = torch.empty(n_ctx, ctx_dim) #context vector for each token, size of context dim\n",
    "\n",
    "            torch.nn.init.normal_(ctx_vectors, std=0.02) #initialize context with random values (mean=0 std=0.02)\n",
    "            prompt_prefix = \" \".join(\"X\" * n_ctx) #placeholder for prompt prefix (\"X X X {class_name}\")\n",
    "\n",
    "        print(f\"Initial context: '{prompt_prefix}'\")\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        self.ctx = torch.nn.Parameter(ctx_vectors) #initialize from context vectors (may be random or initiated from init_ctx) as learnable parameter\n",
    "\n",
    "        #preprocess the class names in a similar manner\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(clip_model.token_embedding.weight.device) #tokenize the prompts and concat back into a single tensor\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts) #get embedding of the entire prompt\n",
    "\n",
    "        #buffer:= part of the state, but not trainable parameters\n",
    "        #-> used in training but not learnable\n",
    "        #saved in save_model(), but ignored in load_model()\n",
    "        #-> we want to use ones created from current class\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :]) #select the first embedding (special token) for all prompts\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1+n_ctx :, :]) #embeddings for all tokens after the context for all prompts\n",
    "\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "        self.name_lens = name_lens\n",
    "        self.class_token_position = class_token_position\n",
    "\n",
    "    def forward(self):\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx\n",
    "\n",
    "        #if CoOp (csc==False), we expand the context tensor to all classes\n",
    "        if ctx.dim() == 2:\n",
    "            #ctx (n_ctx, ctx_dim)\n",
    "            #ctx.unsqueeze(0) (1, n_ctx, ctx_dim)\n",
    "            #ctx.unsqueeze(0).expand(self.n_cls, -1, -1) -> (n_cls, n_ctx, ctx_dim) -> -1 means do not cahnge dims\n",
    "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "\n",
    "        if self.class_token_position == \"end\":\n",
    "            prompts = torch.cat(\n",
    "                [\n",
    "                    prefix, #(n_cls, 1, dim)\n",
    "                    ctx, #(n_cls, n_ctx, dim)\n",
    "                    suffix, #(n_cls, *, dim)\n",
    "                ],  \n",
    "                dim=1 #concat along each prompt\n",
    "            )\n",
    "\n",
    "        elif self.class_token_position == \"middle\":\n",
    "            half_n_ctx = self.n_ctx // 2\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls): #for each class prompt\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i+1, :, :] #prefix of class i\n",
    "                class_i = suffix[i : i+1, :name_len, :] #name of class i\n",
    "                suffix_i = suffix[i : i+1, name_len:, :] #suffix of class i\n",
    "                ctx_i_half1 = ctx[i : i+1, :half_n_ctx, :] #first half of context (before class name)\n",
    "                ctx_i_half2 = ctx[i : i+1, half_n_ctx:, :] #secodn half of context (after class name)\n",
    "                #Note: we use [i:i+1,...] because this way the resulting tensor keep the same dimension [1,x,y]\n",
    "                #if we used [i,...] instead the resulting tensor would be one dimension lower [x,y]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i, #(1, 1, dim)\n",
    "                        ctx_i_half1, #(1, n_ctx//2, dim)\n",
    "                        class_i, #(1, name_len, dim)\n",
    "                        ctx_i_half2, #(1, n_ctx//2, dim)\n",
    "                        suffix_i, #(1, *, dim)\n",
    "                    ],\n",
    "                    dim=1 #concat along each prompt\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0) #concat along the classes\n",
    "\n",
    "        #very similar process for position == \"front\"\n",
    "        elif self.class_token_position == \"front\":\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i+1, :, :]\n",
    "                class_i = suffix[i : i+1, :name_len, :]\n",
    "                suffix_i = suffix[i : i+1, name_len:, :]\n",
    "                ctx_i = ctx[i : i+1, :, :]\n",
    "\n",
    "                prompt = torch.cat([\n",
    "                    prefix_i,\n",
    "                    class_i,\n",
    "                    ctx,\n",
    "                    suffix_i,\n",
    "                    ],\n",
    "                    dim=1\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8eca46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.clip_wrapper import load_clip_model\n",
    "from src.data.dataset import CLASS_NAMES, get_data, base_novel_categories, split_dataset\n",
    "from src.training.trainer import get_cost_function, get_optimizer\n",
    "from src.utils.logging import inspect_model_training, inspect_trainable_parameters\n",
    "from src.training.evaluation import eval, linear_probe_evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5052dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, dataset, categories, batch_size, optimizer, cost_function, device):\n",
    "    total_loss = 0\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    with tqdm(dataloader, desc='Training') as pbar:\n",
    "        for image, target in pbar:\n",
    "            #reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
    "\n",
    "            #transfer relevant data to gpu\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            #get the image features\n",
    "            #image_features = model.encode_image(image)\n",
    "            #image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "            #predict the class by explicit matrix multiplication\n",
    "            logits = model(image)\n",
    "\n",
    "            #calculate the loss\n",
    "            loss = cost_function(logits, target)\n",
    "\n",
    "            #backprop\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #get loss and prediction\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            #update progress bar\n",
    "            pbar.set_postfix(train_loss=loss.item())\n",
    "\n",
    "    return total_loss/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "145b0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewCLIP(torch.nn.Module):\n",
    "    def __init__(self, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
    "        super().__init__()\n",
    "        clip_model, preprocess, _ = load_clip_model()\n",
    "        clip_model = clip_model.float()\n",
    "\n",
    "        self.clip = clip_model\n",
    "        self.preprocess = preprocess\n",
    "        self.prompt_learner = PromptLearner(clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=csc)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.image_encoder(image) #encode the image\n",
    "\n",
    "        prompts = self.prompt_learner() #get the formatted prompts\n",
    "        tokenized_prompts = self.tokenized_prompts \n",
    "        text_features = self.text_encoder(prompts, tokenized_prompts) #encode the text\n",
    "\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        return self.clip.encode_text(text)\n",
    "    \n",
    "    def encode_image(self, image):\n",
    "        return self.clip.encode_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f002a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    lr=1e-2,\n",
    "    wd=5e-4,\n",
    "    momentum=0.9, \n",
    "    classnames=CLASS_NAMES,\n",
    "    n_ctx=8,\n",
    "    ctx_init=\"A picture of a which is a flower\",\n",
    "    class_token_position=\"middle\",\n",
    "    csc=False,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    run = wandb.init(project = \"CoOp-training\", config={\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": lr,\n",
    "        \"weight_decay\": wd,\n",
    "        \"momentum\": momentum,\n",
    "    })\n",
    "\n",
    "    #load the model\n",
    "    model = NewCLIP(\n",
    "        classnames=classnames, n_ctx=n_ctx, ctx_init=ctx_init, class_token_position=class_token_position, csc=csc\n",
    "    ).to(device)\n",
    "\n",
    "    #freeze the model (except prompt learner)\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"prompt_learner\" not in name:\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "    #log number of trainable params\n",
    "    inspect_trainable_parameters(model)\n",
    "\n",
    "    #get datasets\n",
    "    train_set, val_set, test_set = get_data(transform = model.preprocess)\n",
    "    base_classes, novel_classes = base_novel_categories(train_set)\n",
    "    train_base, train_novel = split_dataset(train_set, base_classes)\n",
    "    val_base, _ = split_dataset(val_set, base_classes)\n",
    "    test_base, test_novel = split_dataset(test_set, base_classes)\n",
    "\n",
    "    #initiate training components\n",
    "    optimizer = get_optimizer(model, learning_rate=lr, weight_decay=wd, momentum=momentum)\n",
    "    print(model.parameters())\n",
    "    cost_function = get_cost_function()\n",
    "\n",
    "    #enter training loop\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        #train\n",
    "        model.train()\n",
    "        train_loss = training_step(\n",
    "            model=model,\n",
    "            dataset=train_base,\n",
    "            categories=base_classes,\n",
    "            batch_size=batch_size,\n",
    "            optimizer=optimizer,\n",
    "            cost_function=cost_function,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        #validate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_acc = eval(\n",
    "                model=model,\n",
    "                dataset=val_base,\n",
    "                categories=base_classes,\n",
    "                batch_size=batch_size,\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "        #print ongoing performance\n",
    "        inspect_model_training(model, epoch=epoch, train_loss=train_loss, val_accuracy=val_acc)\n",
    "\n",
    "        #print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Accuracy: {val_acc*100:.2f}%\")\n",
    "\n",
    "        #save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            #torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    \n",
    "    base_accuracy = eval(model=model, dataset=test_base, categories=base_classes, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Base Classes\")\n",
    "    novel_accuracy = eval(model=model, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Novel Classes\")\n",
    "\n",
    "    print()\n",
    "    print(f\"🔍 Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
    "    print(f\"🔍 Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n",
    "\n",
    "    #base_separability = linear_probe_evaluation(model, train_base, test_base, batch_size=32)\n",
    "    #novel_separability = linear_probe_evaluation(model, train_novel, test_novel, batch_size=32)\n",
    "\n",
    "    #print(f\"Base classes separability in embedding: {base_separability}\")\n",
    "    #print(f\"Novel classes separability in embedding: {novel_separability}\")\n",
    "\n",
    "    run.finish()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f18ea73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdigisimon\u001b[0m (\u001b[33mdigisimon-university-of-trento\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/simon/repos/deep-learning-project/wandb/run-20250503_180020-qcfgkyr1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/digisimon-university-of-trento/CoOp-training/runs/qcfgkyr1' target=\"_blank\">gentle-brook-26</a></strong> to <a href='https://wandb.ai/digisimon-university-of-trento/CoOp-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/digisimon-university-of-trento/CoOp-training' target=\"_blank\">https://wandb.ai/digisimon-university-of-trento/CoOp-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/digisimon-university-of-trento/CoOp-training/runs/qcfgkyr1' target=\"_blank\">https://wandb.ai/digisimon-university-of-trento/CoOp-training/runs/qcfgkyr1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial context: 'A picture of a which is a flower'\n",
      "Number of context words (tokens): 8\n",
      "<generator object Module.parameters at 0x7f33218b3ca0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 32/32 [00:12<00:00,  2.63it/s, train_loss=1.36] \n",
      "100%|██████████| 32/32 [00:03<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Train Loss: 1.3109\n",
      "Val Accuracy: 70.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 32/32 [00:12<00:00,  2.51it/s, train_loss=0.306]\n",
      "100%|██████████| 32/32 [00:03<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "Train Loss: 0.8214\n",
      "Val Accuracy: 70.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 32/32 [00:11<00:00,  2.68it/s, train_loss=0.247]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "Train Loss: 0.5397\n",
      "Val Accuracy: 70.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Zero-shot evaluation on Base Classes: 100%|██████████| 20/20 [00:18<00:00,  1.05it/s]\n",
      "🧠 Zero-shot evaluation on Novel Classes: 100%|██████████| 29/29 [00:29<00:00,  1.02s/it]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Base classes accuracy: 71.29%\n",
      "🔍 Novel classes accuracy: 78.24%\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>params_total</td><td>▁</td></tr><tr><td>params_trainable</td><td>▁</td></tr><tr><td>percentage_trainable</td><td>▁</td></tr><tr><td>train_loss</td><td>█▄▁</td></tr><tr><td>val_accuarcy</td><td>▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>params_total</td><td>149624833</td></tr><tr><td>params_trainable</td><td>4096</td></tr><tr><td>percentage_trainable</td><td>0.00274</td></tr><tr><td>train_loss</td><td>0.5397</td></tr><tr><td>val_accuarcy</td><td>0.70588</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-brook-26</strong> at: <a href='https://wandb.ai/digisimon-university-of-trento/CoOp-training/runs/qcfgkyr1' target=\"_blank\">https://wandb.ai/digisimon-university-of-trento/CoOp-training/runs/qcfgkyr1</a><br> View project at: <a href='https://wandb.ai/digisimon-university-of-trento/CoOp-training' target=\"_blank\">https://wandb.ai/digisimon-university-of-trento/CoOp-training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250503_180020-qcfgkyr1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = main()\n",
    "dump_cuda_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bf32968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned context tokens: ['×', '~</w>', '×', '¶', 'H</w>', 'w', 'G</w>', 'Ĥ']\n",
      "\n",
      "Position 0 closest tokens:\n",
      "  âĻ¦: 0.190\n",
      "  no</w>: 0.188\n",
      "  rarely</w>: 0.187\n",
      "  elos</w>: 0.177\n",
      "  known</w>: 0.177\n",
      "\n",
      "Position 1 closest tokens:\n",
      "  reveal</w>: 0.207\n",
      "  (ðŁĵ¸:</w>: 0.197\n",
      "  foto</w>: 0.184\n",
      "  shares</w>: 0.179\n",
      "  endthe: 0.178\n",
      "\n",
      "Position 2 closest tokens:\n",
      "  qualifies</w>: 0.197\n",
      "  rebuilt</w>: 0.197\n",
      "  consisted</w>: 0.184\n",
      "  qualify</w>: 0.181\n",
      "  inevitable</w>: 0.181\n",
      "\n",
      "Position 3 closest tokens:\n",
      "  a</w>: 0.332\n",
      "  my</w>: 0.298\n",
      "  our</w>: 0.273\n",
      "  an</w>: 0.272\n",
      "  the</w>: 0.259\n",
      "\n",
      "Position 4 closest tokens:\n",
      "  saul: 0.194\n",
      "  marcor: 0.193\n",
      "  lew: 0.185\n",
      "  âĿ¤âĿ¤âĿ¤âĿ¤: 0.185\n",
      "  woken</w>: 0.185\n",
      "\n",
      "Position 5 closest tokens:\n",
      "  even</w>: 0.172\n",
      "  afterwards</w>: 0.170\n",
      "  look</w>: 0.167\n",
      "  nell</w>: 0.166\n",
      "  âļªï¸ı: 0.166\n",
      "\n",
      "Position 6 closest tokens:\n",
      "  beauty: 0.180\n",
      "  disupdates</w>: 0.176\n",
      "  costume</w>: 0.174\n",
      "  gladiator</w>: 0.169\n",
      "  fasci: 0.168\n",
      "\n",
      "Position 7 closest tokens:\n",
      "  flower</w>: 0.303\n",
      "  flowers</w>: 0.234\n",
      "  wildflowers</w>: 0.234\n",
      "  ãħĭãħĭ</w>: 0.207\n",
      "  abundant</w>: 0.205\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary of the tokenizer\n",
    "vocab = _tokenizer.encoder\n",
    "# Inverse mapping from token IDs to text\n",
    "id2token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "def decode_embedding(embedding):\n",
    "    \"\"\"Decode an embedding back to text tokens\"\"\"\n",
    "    # Get the most likely token for each position\n",
    "    token_ids = embedding.argmax(dim=-1)\n",
    "    # Convert token IDs back to text\n",
    "    tokens = [id2token.get(id.item(), '?') for id in token_ids]\n",
    "    return tokens\n",
    "\n",
    "# Example usage:\n",
    "context_tokens = decode_embedding(model.prompt_learner.ctx)\n",
    "print(\"Learned context tokens:\", context_tokens)\n",
    "\n",
    "def decode_embedding_cosine(embedding, k=5):\n",
    "    \"\"\"Find k closest tokens for each position in embedding\"\"\"\n",
    "    # Get token embedding matrix\n",
    "    token_embeddings = model.clip.token_embedding.weight\n",
    "    # Normalize embeddings for cosine similarity\n",
    "    normalized_embeddings = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "    normalized_token_embeddings = token_embeddings / token_embeddings.norm(dim=-1, keepdim=True)\n",
    "    # Calculate similarities\n",
    "    similarities = normalized_embeddings @ normalized_token_embeddings.t()\n",
    "    # Get top k similar tokens\n",
    "    topk_similar, topk_indices = similarities.topk(k, dim=-1)\n",
    "    # Convert to tokens\n",
    "    tokens = [[id2token.get(idx.item(), '?') for idx in position_indices] \n",
    "              for position_indices in topk_indices]\n",
    "    return tokens, topk_similar\n",
    "\n",
    "# Example usage:\n",
    "closest_tokens, similarities = decode_embedding_cosine(model.prompt_learner.ctx)\n",
    "for pos, (tokens, sims) in enumerate(zip(closest_tokens, similarities)):\n",
    "    print(f\"\\nPosition {pos} closest tokens:\")\n",
    "    for token, sim in zip(tokens, sims):\n",
    "        print(f\"  {token}: {sim:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
